{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM5YwsRfIOgOWDxVIntS57v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deemalvidarshana/Sinhala-spell-checker/blob/main/spellchecker%20website/Sinhala_Spelling_Checker_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KXeaJSSzGHvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9a7MoJ3GAa5"
      },
      "outputs": [],
      "source": [
        "# train_model.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/AI'\n",
        "DICT_PATH = os.path.join(DRIVE_BASE_PATH, 'Sinhala_Dictionary.text')\n",
        "MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'sinhala_spell_model.h5')\n",
        "TOKENIZER_PATH = os.path.join(DRIVE_BASE_PATH, 'tokenizer.pickle')\n",
        "PARAMS_PATH = os.path.join(DRIVE_BASE_PATH, 'params.json')\n",
        "\n",
        "# Optimized parameters\n",
        "max_len = 12  # Reduced from 15\n",
        "embedding_dim = 32  # Increased from 16\n",
        "lstm_units = 64  # Increased from 16\n",
        "batch_size = 256  # Increased from 128\n",
        "epochs = 5  # Increased from 3\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab, assuming local paths\")\n",
        "\n",
        "print(f\"Loading dictionary from {DICT_PATH}...\")\n",
        "with open(DICT_PATH, 'r', encoding='utf-8') as file:\n",
        "    dictionary = set(file.read().split())\n",
        "print(f\"Loaded {len(dictionary)} words\")\n",
        "\n",
        "# Create tokenizer\n",
        "print(\"Creating tokenizer...\")\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "all_text = ' '.join(dictionary)\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "\n",
        "# Save tokenizer\n",
        "print(\"Saving tokenizer...\")\n",
        "with open(TOKENIZER_PATH, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save parameters\n",
        "params = {\n",
        "    'max_len': max_len,\n",
        "    'vocab_size': len(tokenizer.word_index) + 1\n",
        "}\n",
        "with open(PARAMS_PATH, 'w') as f:\n",
        "    json.dump(params, f)\n",
        "\n",
        "def create_optimized_model(vocab_size):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "        LSTM(lstm_units, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(lstm_units//2),\n",
        "        Dropout(0.2),\n",
        "        Dense(vocab_size//2, activation='relu'),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Prepare training data more efficiently\n",
        "print(\"Preparing training data...\")\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Only use words between 3 and 20 characters\n",
        "for word in dictionary:\n",
        "    if 3 <= len(word) <= 20:\n",
        "        for i in range(1, len(word)):\n",
        "            seq = word[:i]\n",
        "            next_char = word[i]\n",
        "\n",
        "            seq_num = tokenizer.texts_to_sequences([seq])[0]\n",
        "            next_char_num = tokenizer.texts_to_sequences([[next_char]])[0][0]\n",
        "\n",
        "            if len(seq_num) <= max_len:  # Only add sequences within max_len\n",
        "                seq_num = pad_sequences([seq_num], maxlen=max_len)[0]\n",
        "                X.append(seq_num)\n",
        "                y.append(next_char_num)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Training model...\")\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = create_optimized_model(vocab_size)\n",
        "\n",
        "# Split data into train and validation\n",
        "split_idx = int(len(X) * 0.9)\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(f\"Saving model to {MODEL_PATH}...\")\n",
        "model.save(MODEL_PATH)\n",
        "\n",
        "# Print training results\n",
        "final_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "print(f\"\\nFinal Training Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete! Files saved:\")\n",
        "print(f\"1. Model: {MODEL_PATH}\")\n",
        "print(f\"2. Tokenizer: {TOKENIZER_PATH}\")\n",
        "print(f\"3. Parameters: {PARAMS_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-cors"
      ],
      "metadata": {
        "id": "sfzZKnAUZAzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Define paths\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/AI'\n",
        "MODEL_PATH = os.path.join(DRIVE_BASE_PATH, 'sinhala_spell_model.h5')\n",
        "TOKENIZER_PATH = os.path.join(DRIVE_BASE_PATH, 'tokenizer.pickle')\n",
        "PARAMS_PATH = os.path.join(DRIVE_BASE_PATH, 'params.json')\n",
        "\n",
        "# Load tokenizer\n",
        "with open(TOKENIZER_PATH, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load parameters\n",
        "with open(PARAMS_PATH, 'r') as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "max_len = params['max_len']\n",
        "vocab_size = params['vocab_size']\n",
        "\n",
        "# Load model\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "# Define functions\n",
        "def predict_next_char(seq):\n",
        "    seq_num = tokenizer.texts_to_sequences([seq])[0]\n",
        "    seq_num = pad_sequences([seq_num], maxlen=max_len)\n",
        "    pred = model.predict(seq_num, verbose=0)\n",
        "    sorted_indices = np.argsort(pred[0])[::-1]  # Sort predictions by probability\n",
        "    top_predictions = [(tokenizer.index_word[idx], pred[0][idx]) for idx in sorted_indices[:5]]\n",
        "    return top_predictions\n",
        "\n",
        "def correct_sentence(input_text):\n",
        "    words = input_text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "        corrected_word = word\n",
        "        for i in range(1, len(word)):\n",
        "            seq = word[:i]\n",
        "            top_predictions = predict_next_char(seq)\n",
        "            next_char = top_predictions[0][0]  # Most likely next character\n",
        "\n",
        "            if i < len(word) and next_char != word[i]:\n",
        "                corrected_word = seq + next_char\n",
        "        corrected_words.append(corrected_word)\n",
        "\n",
        "    corrected_sentence = ' '.join(corrected_words)\n",
        "    return corrected_sentence\n",
        "\n",
        "# UI Functions\n",
        "def on_check_button_clicked(change):\n",
        "    input_text = text_area.value\n",
        "    corrected_sentence = correct_sentence(input_text)\n",
        "\n",
        "    corrected_output.value = corrected_sentence\n",
        "\n",
        "# UI Components\n",
        "header = widgets.HTML(\"\"\"\n",
        "<h1 style=\"text-align: center; color: #4a90e2; font-family: Arial, sans-serif;\">Sinhala Spelling Checker</h1>\n",
        "\"\"\")\n",
        "\n",
        "# Larger input box\n",
        "text_area = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter your text...',\n",
        "    description='',\n",
        "    layout=widgets.Layout(width='40%', height='150px', margin='0 auto', padding='10px', border_radius='5px')\n",
        ")\n",
        "\n",
        "check_button = widgets.Button(\n",
        "    description='Check Text',\n",
        "    button_style='primary',\n",
        "    tooltip='Click to check spelling',\n",
        "    style={'button_color': '#4a90e2', 'font_weight': 'bold'},\n",
        "    icon='check',\n",
        "    layout=widgets.Layout(margin='20px auto', width='15%')  # Adjusted width to make the button a bit larger\n",
        ")\n",
        "check_button.on_click(on_check_button_clicked)\n",
        "\n",
        "# Larger corrected text box\n",
        "corrected_output = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Corrected text will appear here...',\n",
        "    description='Corrected Text:',\n",
        "    style={'description_width': 'initial', 'font_weight': 'bold'},\n",
        "    layout=widgets.Layout(width='40%', margin='0 auto', padding='10px', border='1px solid #4a90e2', border_radius='5px', background_color='#f2f2f2', font_weight='bold', height='50px')\n",
        ")\n",
        "\n",
        "# Layout\n",
        "app_layout = widgets.VBox([\n",
        "    header,\n",
        "    text_area,\n",
        "    check_button,\n",
        "    corrected_output\n",
        "], layout=widgets.Layout(align_items='center', justify_content='center', padding='20px'))\n",
        "\n",
        "# Display UI\n",
        "display(app_layout)\n"
      ],
      "metadata": {
        "id": "NfekQHXDY3Um"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}